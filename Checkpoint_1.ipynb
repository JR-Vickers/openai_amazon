{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "520ee69d-d1af-4686-a39e-291e3c304160",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "The purpose of this file is to compress our LiDAR .tif file (size: 1.49 GB) down to a smaller size that ChatGPT can interface with (< 20 MB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "150e724e-db35-4341-867b-abe531cd9185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import matplotlib as mpl\n",
    "from rasterio.enums import Resampling\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LightSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41fbe8a6-1bdd-4fe3-be59-f53f4579563a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: /Users/jarrett/Documents/openai_amazon/data/lidar/bronze/preview.png 14.66 MB\n"
     ]
    }
   ],
   "source": [
    "# Set the INFILE variable to the filepath of the LiDAR .tif file you want to compress\n",
    "# Set the OUT_PNG variable to the filepath you want the new, compressed image to appear at.\n",
    "\n",
    "INFILE  = Path(\"/Users/jarrett/Documents/openai_amazon/data/lidar/raw/Inkpkio_NW_LIDAR.tif\")\n",
    "OUT_PNG = Path(\"/Users/jarrett/Documents/openai_amazon/data/lidar/bronze/preview.png\")\n",
    "\n",
    "# Tweak the SCALE variable until the resulting image is 10-20 MB\n",
    "SCALE   = 0.20        # keep 20 % of the width/height\n",
    "CMAP    = \"terrain\"\n",
    "\n",
    "# 1) Read + down-sample\n",
    "with rasterio.open(INFILE) as src:\n",
    "    arr = src.read(\n",
    "        1,\n",
    "        out_shape=(1,\n",
    "                   int(src.height * SCALE),\n",
    "                   int(src.width  * SCALE)),\n",
    "        resampling=Resampling.bilinear)\n",
    "    nodata = src.nodata\n",
    "\n",
    "# 2) Mask no-data\n",
    "arr = np.ma.masked_equal(arr, nodata) if nodata is not None else np.ma.masked_invalid(arr)\n",
    "\n",
    "# 3) Contrast-stretch\n",
    "vmin, vmax = np.percentile(arr.compressed(), (2, 98))\n",
    "norm = np.clip((arr - vmin) / (vmax - vmin), 0, 1)\n",
    "\n",
    "# 4) Hill-shade with proper colormap object\n",
    "ls   = LightSource(azdeg=315, altdeg=45)\n",
    "cmap = mpl.colormaps.get_cmap(CMAP)\n",
    "rgb  = ls.shade(norm, cmap=cmap, vert_exag=1, blend_mode=\"overlay\")\n",
    "\n",
    "# 5) Save\n",
    "plt.imsave(OUT_PNG, rgb)\n",
    "print(\"Saved:\", OUT_PNG, round(OUT_PNG.stat().st_size / 1e6, 2), \"MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c6a2a2-d3d1-49e4-b540-72c588e9b0e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
